{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Sentiment Analysis using TF-IDF model and Linear SVC Data: out of 50000 movie reviews 40000 reviews have been used to train and rest of the 10000 reviews have been used to test the model performance. Preprocessing: 1.removing non-character symbols,\n",
    "\n",
    "add 'not_' before the word where there's an occurence of 'not'. Model: Here I have used TfidfVectorizer from sklearn.feature_extraction.text we are not using bag words model as it will not give any importance to model. Machine Learning: There are 2000 features used in this model and because of that LinearSVC works the best for predicting sentiments with best accuracy. This model is also tested with other learning algorithms like MultinomialNB, RandomForest with 500 estimators and LinearSVC out-performs all of them. Cross Validation: We have used 10 fold cross validation on this model to determine the accuracy of the model on different datasets and we get a mean of 89.80% from all the 10 results. Complete list of Accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rajkumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the dataset\n",
    "from sklearn.datasets import load_files\n",
    "reviews_train = load_files('aclImdb/train/')\n",
    "reviews_test = load_files('aclImdb/test/')\n",
    "X_train,y_train = reviews_train.data,reviews_train.target\n",
    "X_test,y_test = reviews_test.data,reviews_test.target\n",
    "X = X_train + X_test\n",
    "y = np.concatenate([y_train,y_test])\n",
    "X = X[:50000]\n",
    "y = y[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X.pickle','wb') as f:\n",
    "    pickle.dump(X,f)\n",
    "    \n",
    "with open('y.pickle','wb') as f:\n",
    "    pickle.dump(y,f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unpickling dataset\n",
    "X_in = open('X.pickle','rb')\n",
    "y_in = open('y.pickle','rb')\n",
    "X = pickle.load(X_in)\n",
    "y = pickle.load(y_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving the stop words list\n",
    "stop_words = stopwords.words('english')\n",
    "uncheck_words = ['don','won','doesn','couldn','isn','wasn','wouldn','can','ain','shouldn','not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creating the corpus which is the input to TfidfVectorizer\n",
    "corpus = []\n",
    "for i in range(0, len(X)):\n",
    "    antonyms = []\n",
    "    review = re.sub(r'\\W', ' ', str(X[i]))\n",
    "    review = re.sub(r'\\d', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = re.sub(r'br[\\s$]', ' ', review)\n",
    "    review = re.sub(r'\\s+[a-z][\\s$]', ' ',review)\n",
    "    review = re.sub(r'b\\s+', '', review)\n",
    "    review = re.sub(r'\\s+', ' ', review)\n",
    "    word_list = review.split(' ')\n",
    "    newword_list = []\n",
    "    temp_word = ''\n",
    "    for word in word_list:\n",
    "        if temp_word in uncheck_words:\n",
    "            if word not in stop_words:\n",
    "                word = 'not_' + word\n",
    "                temp_word = ''\n",
    "        if word in uncheck_words:\n",
    "            temp_word = word\n",
    "        if word not in uncheck_words:\n",
    "            newword_list.append(word)\n",
    "    review = ' '.join(newword_list)\n",
    "    corpus.append(review)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the weighted BOW model using TF-IDF methodology\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tiv = TfidfVectorizer(max_features = 8000, min_df = 2, norm=\"l2\", use_idf=True, sublinear_tf = True, max_df = 0.6, stop_words = stop_words)\n",
    "X = tiv.fit_transform(corpus).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "text_train, text_test, sent_train, sent_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "# Fitting the Training set to Linear SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC(C = 0.1)\n",
    "classifier.fit(text_train,sent_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(text_train,sent_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(text_train, sent_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pickling classifier\n",
    "with open('svcclassifier.pickle','wb') as f:\n",
    "    pickle.dump(classifier,f)\n",
    "    \n",
    "\n",
    "# Pikling TF-IDF model\n",
    "with open('TFIDF.pickle','wb') as f:\n",
    "    pickle.dump(tiv,f)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predicting the Test set results\n",
    "sent_pred = classifier.predict(text_test)\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(sent_test, sent_pred)\n",
    "\n",
    "print(cm[0][0]+cm[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = text_train, y = sent_train, cv = 10)\n",
    "accuracies.mean()\n",
    "accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
